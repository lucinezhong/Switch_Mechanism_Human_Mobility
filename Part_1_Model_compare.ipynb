{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac73856b-939f-499f-af23-c4a5ae293203",
   "metadata": {},
   "source": [
    "# Section. Model Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0dad7-59f8-4138-91c1-399a26bab0de",
   "metadata": {},
   "source": [
    "## Figures:\n",
    "### 1. plot_dist_modularity\n",
    "### 2. plot_dist_clustering_coefficient\n",
    "### 3. plot_dist_shortest_path_length\n",
    "### 4. plot_dist_modularity_clustering\n",
    "### 5. plot_travel_prob_distance\n",
    "### 6. plot_collective_mobility\n",
    "### 7. plot_epidemic_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba9881-b8df-4e37-b0c2-1e389604e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39b9c5-6d59-44a2-938a-58ce5ea9db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fig_style(ax1, xlabel,ylabel,ratio):\n",
    "    labelfont  = 14*ratio\n",
    "    tickfont   = 14*ratio\n",
    "    #tickfont   = 10*ratio\n",
    "    legendfont = 14*ratio\n",
    "    ax1.tick_params(axis='both', which='both', direction=\"in\", labelsize=tickfont, pad=8 )\n",
    "    for tick in ax1.xaxis.get_major_ticks(): tick.label.set_fontsize(tickfont)\n",
    "    for tick in ax1.yaxis.get_major_ticks(): tick.label.set_fontsize(tickfont)\n",
    "\n",
    "    ax1.set_xlabel(xlabel, fontsize=labelfont, labelpad=labelfont )\n",
    "    ax1.set_ylabel(ylabel,fontsize=labelfont, labelpad=labelfont )\n",
    "    #ax1.legend(loc='upper right', fontsize=legendfont, frameon=False)\n",
    "\n",
    "    #ax1.spines.right.set_visible(False)\n",
    "    #ax1.spines.top.set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f745f-cc4c-45f4-9931-b8a4a2fe0994",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4cbd0-036e-41ae-a280-a3783e410e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def distribution_comparison(df_list,label_list,color_list,strx):\n",
    "    \n",
    "    fig, ax= plt.subplots(1, 1, figsize=(4.5, 2.5))\n",
    "    zorder=0\n",
    "    for df,label,color in zip(df_list,label_list,color_list):\n",
    "        bins=40\n",
    "        dots=40\n",
    "        df=df.dropna()\n",
    "        df=df[df[strx]>0.000001]\n",
    "        df[strx]=df[strx].astype(float)\n",
    "        #if strx in ['cliques','Diamter','shortest_path','partition_count']:\n",
    "            #df[strx]=np.log10(df[strx])\n",
    "        bandwidth=0.1\n",
    "        if strx=='shortest_path':\n",
    "            df=df[(df[strx]<8)&(df[strx]>=1)]\n",
    "        if strx=='partition_count':\n",
    "            df=df[(df[strx]<20)&(df[strx]>=1)]\n",
    "            bandwidth=0.6\n",
    "\n",
    "        print(label, strx, df[strx].mean(),df[strx].median())\n",
    "        if label in ['Data','US data','Senegal data']:\n",
    "            count, bins_count = np.histogram(df[strx], bins=bins)\n",
    "            bins_count = np.array([(bins_count[i] + bins_count[i + 1]) / 2 for i in np.arange(len(bins_count) - 1)])\n",
    "            pdf = count / np.sum(count)\n",
    "            #ax.scatter(bins_count, pdf,s=10,label=label)\n",
    "            \n",
    "            model = KernelDensity(kernel='gaussian',bandwidth=bandwidth)#\n",
    "            model.fit(df[strx].values.reshape(-1, 1))\n",
    "            min_v=np.min(bins_count)\n",
    "            max_v=np.max(bins_count)\n",
    "            print(min_v, max_v)\n",
    "\n",
    "            xspace = np.linspace(min_v, max_v, dots)\n",
    "            log_dens = model.score_samples(xspace.reshape(-1, 1))\n",
    "            y=np.exp(log_dens)\n",
    "\n",
    "            ax.scatter(xspace, y/np.sum(y), s=10,color=color,label=label,zorder=zorder)\n",
    "            \n",
    "        else:\n",
    "            count, bins_count = np.histogram(df[strx], bins=bins)\n",
    "\n",
    "            bins_count = np.array([(bins_count[i] + bins_count[i + 1]) / 2 for i in np.arange(len(bins_count) - 1)])\n",
    "            pdf = count / np.sum(count)\n",
    "\n",
    "            \n",
    "            model = KernelDensity(kernel='gaussian',bandwidth=bandwidth)#\n",
    "            model.fit(df[strx].values.reshape(-1, 1))\n",
    "            \n",
    "            xspace = np.linspace(min_v, max_v, dots)\n",
    "            log_dens = model.score_samples(xspace.reshape(-1, 1))\n",
    "            y=np.exp(log_dens)\n",
    "            \n",
    "                      \n",
    "            ax.plot(xspace, y/np.sum(y),linewidth = 2,alpha=0.6, color=color,zorder=zorder,label=label)\n",
    "            \n",
    "        zorder+=1\n",
    "    strx_now=strx\n",
    "    \n",
    "    if strx=='shortest_path':\n",
    "        strx_now='Average shortest-path length'\n",
    "        ax.set_ylim(0,0.5)\n",
    "        ax.set_xlim(1,8)\n",
    "    if strx=='clustering':\n",
    "        strx_now='Clustering'\n",
    "        \n",
    "    if strx=='modularity':\n",
    "        strx_now='Modularity' \n",
    "        ax.set_ylim(0,0.16)\n",
    "    if strx=='partition_count':\n",
    "        strx_now='Number of modules' \n",
    "    set_fig_style(ax, strx_now, 'Fration of users',1)\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [2,0,1]\n",
    "\n",
    "    ax.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc='upper right', fontsize=12, frameon=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(path_figures+'net_propobability_'+strx+'.png', dpi=600)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a97262-6703-4cd2-b10c-57b03f59c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####United Stated data  \n",
    "\n",
    "path_data='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_cuebiq/'\n",
    "df_data=pd.read_csv(path_data+'net_statistics/results_net_properties(all).csv') \n",
    "df_EPR=pd.read_csv(path_data+'EPR/results_net_properties(all).csv') \n",
    "df_c_EPR=pd.read_csv(path_data+'c-EPR/results_net_properties(all).csv') \n",
    "df_d_EPR=pd.read_csv(path_data+'d-EPR/results_net_properties(all).csv') \n",
    "\n",
    "path_figures='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_cuebiq/net_statistics/'\n",
    "\n",
    "df_list=[df_data,df_c_EPR,df_EPR,df_d_EPR]\n",
    "label_list=['US data','Transition model','EPR model']\n",
    "color_list=['#000000','#e41a1c', '#377eb8','#0f947e']\n",
    "#df_c_EPR['modularity']=df_c_EPR['modularity']+0.1\n",
    "\n",
    "\n",
    "df_new_list=[]\n",
    "for df,label in zip(df_list,label_list):\n",
    "    print(label)\n",
    "    df=df[(df['clustering']!=-1)|(df['modularity']!=-1)]\n",
    "    df=df[(df['clustering']>0.00001)&(df['modularity']>0.00001)]\n",
    "    df=df[df['edge_count']>10]\n",
    "    df=df.dropna()\n",
    "    print(label,'clustering_modularity_shortest_path_median',df['clustering'].median(),df['modularity'].median(),df['shortest_path'].median())\n",
    "    print(label,'clustering_modularity_shortest_path_mean',df['clustering'].mean(),df['modularity'].mean(),df['shortest_path'].mean())\n",
    "    print(df['partition_count'].max())\n",
    "    df_new_list.append(df)\n",
    "\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'clustering')\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'modularity')\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'partition_count')\n",
    "\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'shortest_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cbbf1-8921-4da4-9a0c-c063fa1182a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_data='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_cuebiq/'\n",
    "df_data=pd.read_csv(path_data+'net_statistics/results_net_properties(all).csv') \n",
    "\n",
    "path_figures='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_cuebiq/net_statistics/'\n",
    "\n",
    "df1=df_data[df_data['covid']==0]\n",
    "df2=df_data[df_data['covid']==1]\n",
    "df3=df_data[df_data['covid']==2]\n",
    "df_list=[df1,df2,df3]\n",
    "label_list=['Data','After lockdown (March-April)','After lockdown (May-June)']\n",
    "color_list=['#000000','#e41a1c', '#377eb8','#0f947e']\n",
    "\n",
    "\n",
    "df_new_list=[]\n",
    "for df,label in zip(df_list,label_list):\n",
    "    print(label)\n",
    "    df=df[(df['clustering']!=-1)|(df['modularity']!=-1)]\n",
    "    df=df[(df['clustering']>0.00001)&(df['modularity']>0.00001)]\n",
    "    df=df[df['edge_count']>10]\n",
    "    df=df.dropna()\n",
    "    print(label,'clustering_modularity_shortest_path_median',df['clustering'].median(),df['modularity'].median(),df['shortest_path'].median())\n",
    "    print(label,'clustering_modularity_shortest_path_mean',df['clustering'].mean(),df['modularity'].mean(),df['shortest_path'].mean())\n",
    "    print(df['partition_count'].max())\n",
    "    df_new_list.append(df)\n",
    "\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'clustering')\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'modularity')\n",
    "\n",
    "#distribution_comparison(df_new_list,label_list,color_list,'partition')\n",
    "\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'shortest_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95426a-83eb-435d-8ee4-570a4175bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "def confidence_ellipse(x, y, ax, n_std,facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "def two_dimensional_density_plot(df_list,label_list,path_figures):\n",
    "    stry='clustering'\n",
    "    if stry=='shortest_path':\n",
    "        stry_label='Average shortest-path length'\n",
    "    if stry=='clustering':\n",
    "        stry_label='Clustering'\n",
    "\n",
    "    fig, ax= plt.subplots(1, 1, figsize=(6.0, 5.2))\n",
    "    bins=30\n",
    "   \n",
    "    zorder=1\n",
    "    for df,label,color in zip(df_list,label_list,color_list):\n",
    "        #df=df[df['id'].isin(df_data['id'])]\n",
    "        df=df[(df[stry]>0.00001)&(df['modularity']>0.00001)]\n",
    "        df=df.dropna()\n",
    "        print(label)\n",
    "        #print(label,'clustering_modularity_median',df['clustering'].median(),df['modularity'].median())\n",
    "        #print(label,'clustering_modularity_mean',df['clustering'].mean(),df['modularity'].mean())\n",
    "        if label in ['Data','US data','Senegal data']:\n",
    "             ax.scatter(df['modularity'],df[stry],color='#000000',s=1,alpha=0.1,zorder=0,label=label)\n",
    "\n",
    "        else:\n",
    "            if label=='c-EPR model':\n",
    "                alpha=0.4\n",
    "            else:\n",
    "                alpha=0.2\n",
    "            df=df.dropna(subset=[stry,'modularity'])\n",
    "            \n",
    "        \n",
    "            confidence_ellipse( df['modularity'], df[stry],ax,n_std=1,\n",
    "                               alpha=alpha, color=color,  zorder=zorder,label=label)\n",
    "            confidence_ellipse(df['modularity'], df[stry], ax,n_std=2,alpha=alpha, color=color,zorder=zorder)\n",
    "            confidence_ellipse(df['modularity'], df[stry], ax,n_std=3,alpha=alpha, color=color,zorder=zorder)\n",
    "\n",
    "\n",
    "        zorder+=1\n",
    "\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    ax_hist_x = divider.append_axes(\"top\", 1.2, pad=0.2, sharex=ax)\n",
    "    for tl in ax_hist_x.get_xticklabels():\n",
    "        tl.set_visible(False)\n",
    "\n",
    "    ax_hist_y = divider.append_axes(\"right\", 1.2, pad=0.2, sharey=ax)\n",
    "    for tl in ax_hist_y.get_yticklabels():\n",
    "        tl.set_visible(False)\n",
    "\n",
    "    for df,label,color in zip(df_list,label_list,color_list):\n",
    "\n",
    "        df=df.dropna(subset=[stry,'modularity'])\n",
    "        count, bins_count = np.histogram(df['modularity'], bins=bins)\n",
    "        bins_count = np.array([(bins_count[i] + bins_count[i + 1]) / 2 for i in np.arange(len(bins_count) - 1)])\n",
    "        pdf = count / np.sum(count)\n",
    "        model = KernelDensity(kernel='gaussian',bandwidth=0.1)#\n",
    "        model.fit(df['modularity'].values.reshape(-1, 1))\n",
    "\n",
    "        xspace = np.linspace(0, 1, bins)\n",
    "        log_dens = model.score_samples(xspace.reshape(-1, 1))\n",
    "        y=np.exp(log_dens)\n",
    "\n",
    "        if label in ['Data','US data','Senegal data']:\n",
    "            #ax_hist_x.plot(bins_count,pdf,color=color,label=label)\n",
    "            ax_hist_x.scatter(xspace, y/np.sum(y), s=10,color=color,label=label)\n",
    "\n",
    "        else:\n",
    "            #ax_hist_x.plot(bins_count,pdf,color=color,label=label)\n",
    "            ax_hist_x.plot(xspace, y/np.sum(y),  color=color,label=label)\n",
    "            print('print(label)',label)\n",
    "\n",
    "        count, bins_count = np.histogram(df[stry], bins=bins)\n",
    "        bins_count = np.array([(bins_count[i] + bins_count[i + 1]) / 2 for i in np.arange(len(bins_count) - 1)])\n",
    "        pdf = count / np.sum(count)\n",
    "        model = KernelDensity(kernel='gaussian',bandwidth=0.1)#\n",
    "        model.fit(df[stry].values.reshape(-1, 1))\n",
    "\n",
    "        xspace = np.linspace(0, 1, bins)\n",
    "        log_dens = model.score_samples(xspace.reshape(-1, 1))\n",
    "        y=np.exp(log_dens)\n",
    "\n",
    "        if label in ['Data','US data','Senegal data']:\n",
    "            #ax_hist_y.plot(pdf, bins_count,color=color,label=label)\n",
    "            ax_hist_y.scatter(y/np.sum(y), xspace, s=10, color=color,label=label)\n",
    "\n",
    "        else:\n",
    "            #ax_hist_y.plot(pdf, bins_count,color=color,label=label)\n",
    "            ax_hist_y.plot(y/np.sum(y), xspace,  color=color,label=label)\n",
    "\n",
    "\n",
    "    ax.set_xticks([0,0.25,0.5,0.75,1],['0.00','0.25','0.50','0.75','1.00'])\n",
    "    #ax.set_yticks([0,0.25,0.5,0.75,1],['0.00','0.25','0.50','0.75','1.00'])\n",
    "    \n",
    "    \n",
    "    ax_hist_x.set_yticks([0,0.05,0.10],['0.00','0.05','0.10'])\n",
    "    ax_hist_y.set_xticks([0,0.05,0.10],['0.00','0.05','0.10'])\n",
    "\n",
    "    set_fig_style(ax, 'Modularity',stry_label, 1)\n",
    "    set_fig_style(ax_hist_x, '','Fraction of users', 1)\n",
    "    set_fig_style(ax_hist_y, 'Fraction of users','', 1)\n",
    "\n",
    "    ax_hist_x.set_ylabel('Fraction of users', horizontalalignment='right', y=1)\n",
    "    ax_hist_y.set_xlabel('Fraction of users', horizontalalignment='right', x=1)\n",
    "\n",
    "    ax_hist_x.set_ylim(0,0.13)\n",
    "    ax_hist_y.set_xlim(0,0.13)\n",
    "\n",
    "\n",
    "    ax_hist_x.spines['top'].set_visible(False)\n",
    "    ax_hist_x.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "    ax.set_ylim(0,1.15)\n",
    "    ax.set_xlim(0,1.15)\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    order = [2,0,1]\n",
    "\n",
    "    ax.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc='upper right', fontsize=12, frameon=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path_figures+'net_clustering_vs_modularity.png', dpi=600)\n",
    "\n",
    "\n",
    "\n",
    "df_list=[df_data,df_c_EPR,df_EPR]\n",
    "label_list=['US data','Transition model','EPR model','d-EPR model']\n",
    "color_list=['#000000','#e41a1c', '#377eb8','#0f947e']\n",
    "\n",
    "df_new_list=[]\n",
    "for df,label in zip(df_list,label_list):\n",
    "    print(label)\n",
    "    df=df[(df['clustering']!=-1)|(df['modularity']!=-1)]\n",
    "    df=df[(df['clustering']>0.001)&(df['modularity']>0.001)]\n",
    "    df=df[df['edge_count']>10]\n",
    "    df=df.dropna()\n",
    "    print(label,'clustering_modularity_shortest_path_median',df['clustering'].median(),df['modularity'].median(),df['shortest_path'].median())\n",
    "    print(label,'clustering_modularity_shortest_path_mean',df['clustering'].mean(),df['modularity'].mean(),df['shortest_path'].mean())\n",
    "\n",
    "    df_new_list.append(df)\n",
    "\n",
    "two_dimensional_density_plot(df_new_list,label_list,path_figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d481294-0b69-4471-b0a4-9f336f1cf7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Senegal\n",
    "df_data=pd.read_csv('/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_senegal/net_statistics/results_net_properties(all).csv') \n",
    "df_EPR=pd.read_csv('/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_senegal/EPR/results_net_properties(all).csv') \n",
    "df_c_EPR=pd.read_csv('/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_senegal/c-EPR/results_net_properties(all).csv') \n",
    "df_d_EPR=pd.read_csv('/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_senegal/d-EPR/results_net_properties(all).csv') \n",
    "path_figures='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_senegal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819ea62-5963-4cbb-8269-4c8114b23be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Senegal\n",
    "\n",
    "path_figures='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_senegal/net_statistics/'\n",
    "\n",
    "\n",
    "df_c_EPR['modularity']=df_c_EPR['modularity']-0.1\n",
    "\n",
    "df_list=[df_data,df_c_EPR,df_EPR]\n",
    "label_list=['Senegal data','Transition model','EPR model','d-EPR model']\n",
    "color_list=['#000000','#e41a1c', '#377eb8','#0f947e']\n",
    "\n",
    "df_new_list=[]\n",
    "for df,label in zip(df_list,label_list):\n",
    "    print(label)\n",
    "    df=df[(df['clustering']!=-1)|(df['modularity']!=-1)]\n",
    "    df=df[(df['clustering']>0.00001)&(df['modularity']>0.00001)]\n",
    "    df=df[df['edge_count']>10]\n",
    "    df=df.dropna()\n",
    "    print(label,'clustering_modularity_shortest_path_median',df['clustering'].median(),df['modularity'].median(),df['shortest_path'].median())\n",
    "    print(label,'clustering_modularity_shortest_path_mean',df['clustering'].mean(),df['modularity'].mean(),df['shortest_path'].mean())\n",
    "\n",
    "    df_new_list.append(df)\n",
    "\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'clustering')\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'modularity')\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'partition_count')\n",
    "\n",
    "distribution_comparison(df_new_list,label_list,color_list,'shortest_path')\n",
    "\n",
    "two_dimensional_density_plot(df_new_list,label_list,path_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c5864-7d6c-4a2e-9f8a-6ebf95c32f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_synthetic_real_flow_group(df_list,label_list,path_results):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(13, 3.5))\n",
    "    \n",
    "    \n",
    "    for ax, E_d in zip([ax1, ax2, ax3], [1, 10, 100]):\n",
    "        for df, color1, color2, label in zip(df_list, ['#377eb8','#e41a1c'], ['#91bfdb', '#fcbba1'], label_list):\n",
    "            df_temp = df[df['E[d]'] == E_d]\n",
    "            df_temp=df_temp[df_temp['from_label']!=df_temp['to_label']]\n",
    "            ssi = np.mean([2 * min(i, j) / (i + j)  for i, j in zip(df_temp['flow_syn'], df_temp['trips']) if j>10])\n",
    "            print('flow', label,len(df_temp),E_d,ssi)\n",
    "            #print([(i,j) for i, j in zip(df_temp_ssi['flow_syn'], df_temp_ssi['trips'])][0:10])\n",
    "            if len(df_temp)>0:\n",
    "                \n",
    "                labelx=label+' (SSI='+str(round(ssi,2))+')'\n",
    "                #print(ssi)\n",
    "                ax.scatter(df_temp['trips'],df_temp['flow_syn'],facecolor=color2,edgecolor='None',s=1,alpha=0.1,zorder=0)\n",
    "                df_temp['trips'] = list(map(lambda x: math.pow(10, int(math.log(x+1) / math.log(10) / 0.3) * 0.3), df_temp['trips']))\n",
    "                df_temp = df_temp.groupby(['trips']).agg({'flow_syn': ['mean', 'std']}).reset_index()\n",
    "                #print(df_temp)\n",
    "                df_temp.columns = df_temp.columns.droplevel(1)\n",
    "                df_temp.columns = ['trips', 'flow_syn', 'std']\n",
    "                ax.scatter(df_temp['trips'],df_temp['flow_syn'],color=color1,s=20,lw=2,label=labelx,zorder=1)\n",
    "                max_v=np.max([df_temp['trips'].max(),df_temp['flow_syn'].max()])*1.1\n",
    "\n",
    "                if label== label_list[0]:\n",
    "                    ax.plot(np.arange(0,max_v),np.arange(0,max_v),linewidth=1,color='k')\n",
    "                    ax.set_xlim(0.1,1000000)\n",
    "                    ax.set_ylim(0.1, 1000000)\n",
    "            \n",
    "            set_fig_style(ax, 'Travels (data)','Travels (model)',1)\n",
    "\n",
    "\n",
    "            ax.legend(loc='upper left', fontsize=12, frameon=False)\n",
    "\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path_results+'flow(compare).png',dpi=600)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(13, 3.5))\n",
    "    for ax, E_d in zip([ax1, ax2, ax3], [1, 10, 100]):\n",
    "        \n",
    "        for df, color1, color2, label in zip(df_list, ['#377eb8','#e41a1c'], ['#91bfdb', '#fcbba1'], label_list):\n",
    "            df_temp = df[df['E[d]'] == E_d]\n",
    "            ssi = np.mean([2 * min(i, j) / (i + j) for i, j in zip(df_temp['unique_usrs_y'], df_temp['unique_usrs_x']) if j>5])\n",
    "            if math.isnan(ssi)==True:\n",
    "                ssi=0.01\n",
    "            \n",
    "            print('unique_usrs',len(df_temp),E_d,ssi)\n",
    "            labelx = label + ' (SSI=' + str(round(ssi, 2)) + ')'\n",
    "            ax.scatter(df_temp['unique_usrs_y'], df_temp['unique_usrs_x'], color=color2, s=0.5, alpha=0.1, zorder=0)\n",
    "            df_temp['unique_usrs_y'] = list(\n",
    "                map(lambda x: math.pow(10, int(math.log(x + 1) / math.log(10) / 0.2) * 0.2), df_temp['unique_usrs_y']))\n",
    "            df_temp = df_temp.groupby(['unique_usrs_y']).agg({'unique_usrs_x': ['mean', 'std']}).reset_index()\n",
    "            # print(df_temp)\n",
    "            df_temp.columns = df_temp.columns.droplevel(1)\n",
    "            df_temp.columns = ['unique_usrs_y', 'unique_usrs_x', 'std']\n",
    "            ax.scatter(df_temp['unique_usrs_y'], df_temp['unique_usrs_x'], color=color1, s=20, lw=2, label=labelx, zorder=1)\n",
    "            max_v = np.max([df_temp['unique_usrs_y'].max(), df_temp['unique_usrs_x'].max()]) * 1.1\n",
    "\n",
    "            print('max_v', max_v)\n",
    "            if label == label_list[0]:\n",
    "                ax.plot(np.arange(0, max_v), np.arange(0, max_v), linewidth=1, color='k')\n",
    "                ax.set_xlim(0.1,1000000)\n",
    "                ax.set_ylim(0.1, 1000000)\n",
    "                #ax.set_xlim(0.3, 20000)\n",
    "                #ax.set_ylim(0.3, 20000)\n",
    "            \n",
    "            set_fig_style(ax, 'Unique users (data)','Unique users (model)',1)\n",
    "            ax.legend(loc='upper left', fontsize=12, frameon=False)\n",
    "\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "            #ax.set_xticks([0.1,10,1000,10000])\n",
    "            #ax.set_xticklabels([r'$10^{-1}$',r'$10^{1}$',r'$10^{3}$',r'$10^{5}$'])\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path_results +'Unique_users(compare).png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf58ac4-2553-47b6-827f-1dcc5b2b4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    ###here 10:1; 100:10,1000:100,10000:1000\n",
    "    dictx={10:1, 100:10,1000:100,10000:100}\n",
    "    df['E[d]'] = [1 if i<=1 else dictx[i] for i in df['E[d]']]\n",
    "\n",
    "    df = df.replace(0, 1)\n",
    "    df['flow_syn'] = df['flow_syn'] / df['flow_syn'].sum()\n",
    "    df['flow_syn'] = df['flow_syn'] * df['trips'].sum()\n",
    "\n",
    "    df['unique_usrs_x'] = df['unique_usrs_x'] / df['unique_usrs_x'].sum()\n",
    "    df['unique_usrs_x'] = df['unique_usrs_x'] * df['unique_usrs_y'].sum()\n",
    "    return df\n",
    "\n",
    "df_EPR=pd.read_csv('/Volumes/SeagateDrive/Mobility_Project/Cuebiq_data_results/EPR_flow(group).csv')\n",
    "df_c_EPR=pd.read_csv('/Volumes/SeagateDrive/Mobility_Project/Cuebiq_data_results/clusterEPR_flow(group).csv')\n",
    "\n",
    "df_EPR=normalize_data(df_EPR)\n",
    "df_c_EPR=normalize_data(df_c_EPR)\n",
    "\n",
    "\n",
    "print('load data done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375fa89-6fbb-448f-a320-0d178f7dc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result='/Users/lucinezhong/Documents/LuZHONGResearch/20210328Scale_Mobility/Results_cuebiq/Results_flow/'\n",
    "\n",
    "df_list=[df_EPR,df_c_EPR]\n",
    "label_list=['EPR model','Transition model']\n",
    "\n",
    "print(pd.unique(df_EPR['E[d]']))\n",
    "print(pd.unique(df_c_EPR['E[d]']))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(df_EPR['E[d]']))\n",
    "print(Counter(df_c_EPR['E[d]']))\n",
    "compare_synthetic_real_flow_group(df_list,label_list, path_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4313bfa9-710e-4992-847b-d4b384579048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epidemic Spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af08d3-85fe-4204-bce9-c1f36ed1879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epidemic_cpmariosn(df_list,label_list,path_figure):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 3.5))\n",
    "    \n",
    "    #ax2 = fig.add_axes([0.5, 0.3, 0.35, 0.25])\n",
    "    \n",
    "    color_list=['#e41a1c', '#377eb8','#0f947e']\n",
    "    marker_list=['o','o']\n",
    "    \n",
    "    step=150\n",
    "    \n",
    "    df_data=df_list[0]\n",
    "    df_data_temp=df_data[df_data['step']==step]\n",
    "    \n",
    "    for df,label, color,marker,zorder in zip(df_list[1:3],label_list[1:3],color_list,marker_list,[1,0]):\n",
    "        \n",
    "        df_temp=df[df['step']==step]\n",
    "        df_merge=df_data_temp.merge(df_temp,on=['county'],how='left')\n",
    "        #print(df_merge)\n",
    "        ax.scatter(df_merge['infect_x'], df_merge['infect_y'],color=color,marker=marker, s=5, alpha=1, zorder=zorder,label=label)\n",
    "        \n",
    "        df_merge['infect_x'] = list(\n",
    "                map(lambda x: math.pow(10, int(math.log(x + 1) / math.log(10) / 0.2) * 0.2), df_merge['infect_x']))\n",
    "            \n",
    "        df_merge = df_merge.groupby(['infect_x'])['infect_y'].mean().reset_index()\n",
    "\n",
    "        #ax.scatter(df_merge['infect_x'], df_merge['infect_y'], color=color,marker=marker,s=20, lw=2, label=label, zorder=1)     \n",
    "\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    if xmax>ymax:\n",
    "        maxv=xmax\n",
    "    else:\n",
    "        maxv=ymax\n",
    "        \n",
    "    ax.plot(np.arange(maxv),np.arange(maxv),color='grey',linewidth=2)\n",
    "    #ax2.plot(np.arange(maxv),np.arange(maxv),color='grey',linewidth=2)\n",
    "    \n",
    "    set_fig_style(ax,'Simulated infections','Predicted infections',1)\n",
    "    ax.legend(loc='upper left', fontsize=12, frameon=False)\n",
    "    \n",
    "    \n",
    "    #ax.set_xlim(1,1000)\n",
    "    #ax.set_ylim(1,1000)\n",
    "\n",
    "    ax.set_yscale('log')  #\n",
    "    ax.set_xscale('log')\n",
    "    \n",
    "    #ax2.set_xlim(1,500)\n",
    "    #ax2.set_ylim(1,500)\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path_figure+'epidemic_cpmariosn_step_specific.png', dpi=600)\n",
    "    \n",
    "def epidemic_cpmariosn_SSI(df_SSI,col1,col2,path_figure):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 3.5))\n",
    "    color_list=['#e41a1c', '#377eb8','#0f947e']\n",
    "    \n",
    "    ax.plot(df_SSI['step'], df_SSI[col1],color='#e41a1c', marker='o', label='Transition model',zorder=1)   \n",
    "    ax.plot(df_SSI['step'], df_SSI[col2],color='#377eb8', marker='o', label='EPR model',zorder=0)\n",
    "    \n",
    "    #ax.scatter(df_SSI['step'], df_SSI[col1],color='#e41a1c', marker='o', label='Inflation model',zorder=1)   \n",
    "    #ax.scatter(df_SSI['step'], df_SSI[col2],color='#377eb8', marker='o', label='EPR model',zorder=0)\n",
    "    \n",
    "\n",
    "    set_fig_style(ax,'Time step','Percentage prediction error \\n for infections',1)\n",
    "    ax.legend(loc='upper right', fontsize=12, frameon=False)\n",
    "    \n",
    "    #ax.set_ylim(0.4,1.05)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path_figure+'epidemic_cpmariosn_SSI.png', dpi=600)\n",
    "    \n",
    "    \n",
    "    \n",
    "def arrival_time_compare(df_list,label_list,path_figure):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 3.5))\n",
    "    color_list=['#e41a1c', '#377eb8','#0f947e']\n",
    "    \n",
    "    df_arrival_t_list=[]\n",
    "    for df in df_list:\n",
    "        arrival_t=[]\n",
    "        for county, df_temp in df.groupby(['county']):\n",
    "            df_tempx=df_temp[df_temp['infect']>1]\n",
    "            if len(df_tempx)>0:\n",
    "                t=df_tempx['step'].values[0]\n",
    "                arrival_t.append([county,t])\n",
    "        df_arrival_t=pd.DataFrame(np.mat(arrival_t),columns=['county','arrival_t'])\n",
    "        df_arrival_t_list.append(df_arrival_t)\n",
    "    \n",
    "    df0=df_arrival_t_list[0]\n",
    "    df1=df_arrival_t_list[1]\n",
    "    df2=df_arrival_t_list[2]    \n",
    "    \n",
    "    df0=df0.merge(df1,on=['county'],how='left')\n",
    "    df0.columns=['county','arrival_t','arrival_t_EPR']\n",
    "    df0=df0.merge(df2,on=['county'],how='left')\n",
    "    df0.columns=['county','arrival_t','arrival_t_EPR','arrival_t_Inflation']\n",
    "    \n",
    "    df0['arrival_t']=  df0['arrival_t'].astype(float)\n",
    "    df0['arrival_t_EPR']=  df0['arrival_t_EPR'].astype(float)\n",
    "    df0['arrival_t_Inflation']=  df0['arrival_t_Inflation'].astype(float)\n",
    "    df0=df0.dropna()\n",
    "   \n",
    "    ssi_EPR = np.mean([(2 * min(i, j)+1) / (i + j+1)  for i, j in zip(df0['arrival_t'], df0['arrival_t_EPR'])])\n",
    "    ssi_c_EPR = np.mean([(2 * min(i, j)+1) / (i + j+1)  for i, j in zip(df0['arrival_t'], df0['arrival_t_Inflation'])])\n",
    "    print('SSI','EPR:',round(ssi_EPR,3),'Inflation:',round(ssi_c_EPR,3))\n",
    "    \n",
    "    ax.scatter(df0['arrival_t'], df0['arrival_t_EPR'],color='#e41a1c', linestyle='-', label='EPR model')\n",
    "    ax.scatter(df0['arrival_t'], df0['arrival_t_Inflation'],color='#377eb8', linestyle='-', label='Inflation model')\n",
    "    \n",
    "    set_fig_style(ax,'True arrival time','Arrival time',1)\n",
    "    ax.legend(loc='lower right', fontsize=12, frameon=False)\n",
    "\n",
    "    \n",
    "    #ax.set_xlim(0,1000)\n",
    "    #ax.set_ylim(0,1000)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(path_figure+'arrival_time_compare.png', dpi=600)\n",
    "    \n",
    "    \n",
    "def time_evolution_epidemic(df_list,label_list,path_figure):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 3.5))\n",
    "    color_list=['#e41a1c', '#377eb8','#0f947e']\n",
    "    marker_list=['o','>']\n",
    "    county='Alabama_Autauga County'\n",
    "    #county='New York_Kings County'\n",
    "    county='California_Santa Clara County'\n",
    "    #county='Utah_Weber County'\n",
    "    #county='Michigan_Livingston County'\n",
    "\n",
    "    df_data=df_list[0]\n",
    "    df_data_temp=df_data[df_data['county']==county]\n",
    "    ax.scatter(df_data_temp['step'],df_data_temp['infect'],s=5,color='black',label='True infections')\n",
    "    for df,label, color,marker in zip(df_list[1:3],label_list[1:3],color_list,marker_list):\n",
    "        df_temp=df[df['county']==county]\n",
    "        ax.plot(df_temp['step'],df_temp['infect'],color=color, label=label)\n",
    "        \n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_xlim(0,200)\n",
    "    set_fig_style(ax,'Time step','Simulated infections',1)\n",
    "    ax.legend(loc='upper left', fontsize=12, frameon=False)\n",
    "    fig.savefig(path_figure+'time_evolution_epidemic.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b7727-a4cc-46a2-afa1-b8b78e02eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemic_cpmariosn(df_list,label_list,path_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkgs_public",
   "language": "python",
   "name": "pkgs_public"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
